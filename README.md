# Natural Language Processing with Disaster Tweets

This project is based on the Kaggle competition [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/overview). The goal of the competition is to build a model that can predict whether a given tweet is about a real disaster or not.

## Project Overview

This project applies various Natural Language Processing (NLP) techniques to analyze and classify tweets. The key stages of the project include:

1. **Data Exploration**: Analyzing the structure, distribution, and characteristics of the dataset.
2. **Data Cleaning and Preprocessing**: Handling missing data, text normalization, tokenization, and removing noise from the tweets.
3. **Feature Engineering**: Extracting meaningful features from the tweet text, such as TF-IDF scores, word embeddings, or sentiment analysis.
4. **Model Building**: Experimenting with different machine learning models including Logistic Regression, Random Forest, and advanced models like BERT.
5. **Model Evaluation**: Assessing model performance using accuracy, precision, recall, F1-score, and other relevant metrics.
6. **Submission**: Generating predictions on the test data for submission to the Kaggle competition.

## Files in the Repository

- `Natural_Language_Processing_with_Disaster_Tweets.ipynb`: The main Jupyter notebook containing the entire workflow, from data exploration to model evaluation.
- `natural_language_processing_with_disaster_tweets.py`: A script version of the project, including all the key steps for running the project in a non-notebook environment.
- `source_data`: Directory with training and test data.
- `results`: Directory with predictions generated by trained models with use of this project.

## How to Run

1. **Clone the repository**:
    ```bash
    git clone https://github.com/your-username/your-repo-name.git
    cd your-repo-name
    ```

2. **Install the required packages**:
    ```bash
    pip install -r requirements.txt
    ```

3. **Run the notebook or script**:
   - For the notebook, use Jupyter Notebook or Jupyter Lab:
     ```bash
     jupyter notebook Natural_Language_Processing_with_Disaster_Tweets.ipynb
     ```
   - For the script:
     ```bash
     python natural_language_processing_with_disaster_tweets.py
     ```

## Utilized Libraries

This project leverages the `nlp_utils` package, which was developed in another project of mine. `nlp_utils` provides a set of convenient utilities for text preprocessing, feature extraction, and more, making it easier to implement various NLP techniques efficiently.

## Results

The final model achieves a high level of accuracy in classifying tweets, making it suitable for real-world applications where identifying disaster-related content is crucial.

## Contributions

Feel free to fork the repository and submit pull requests. Contributions are welcome!

## License

This project is licensed under the MIT License.


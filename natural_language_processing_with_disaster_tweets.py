# -*- coding: utf-8 -*-
"""Natural Language Processing with Disaster Tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8d3FrUt-uQG96aM0SiToju9qaDVjYDU

# Natural Language Processing with Disaster Tweets

[Link to Kaggle competition](https://www.kaggle.com/competitions/nlp-getting-started/overview)

## 1. Data Exploration

### 1.1. Firt look on the data
"""

import pandas as pd

df = pd.read_csv('train.csv')
df.head()

df.info()

df['keyword'].value_counts()

df['location'].value_counts()

df['target'].value_counts()

"""### 1.2. Target Class Distribution"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.rcParams['figure.figsize'] = (10, 5)
plt.rcParams['figure.dpi'] = 100

df.shape

sns.countplot(x='target', data=df)
plt.title('Real or false disaster tweets')

df['target'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Real or false disaster tweets')

"""### 1.2.1 Number of characters in Tweets distribution"""

!pip install git+https://github.com/muchlinski/nlp_utils.git

import nlp_utils

df = nlp_utils.get_basic_features(df)
df.head()

sns.histplot(df['char_counts'])
plt.title('Number of characters in Tweets distribution')

sns.kdeplot(df['char_counts'], fill=True)
plt.title('Number of characters in Tweets distribution')

sns.kdeplot(df[df['target']==1]['char_counts'], fill=True, label='Real disaster tweets', color='red')
sns.kdeplot(df[df['target']==0]['char_counts'], fill=True, label='False disaster tweets', color='green')
plt.legend()

sns.catplot(y='char_counts', data=df, kind='violin', col='target')

"""### 1.2.2. Number of words, avg words length, stop words distribution in Tweets"""

def plot_column_distribution(df, col):
    plt.rcParams['figure.figsize'] = (10, 5)
    plt.rcParams['figure.dpi'] = 100
    sns.kdeplot(df[df['target']==1][col], fill=True, label='Disaster Tweets' ,color='red')
    sns.kdeplot(df[df['target']==0][col], fill=True, label='Non-Disaster Tweets', color='green')
    plt.title(f"Distribution of values in column {col}")
    plt.legend()

plot_column_distribution(df, 'word_counts')

plot_column_distribution(df, 'avg_wordlength')

plot_column_distribution(df, 'stopwords_counts')

"""### 1.2.3. Most and Least Common Words"""

freqs = nlp_utils.get_value_counts(df, 'text')
top20 = freqs.head(20)
top20

plt.bar(top20.index, top20.values)
plt.xticks(rotation=60)
plt.show()

least20 = freqs.tail(20)
least20

"""## 2. One-Shot Data Cleaning"""

df = pd.read_csv('train.csv')
df['text'] = df['text'].apply(lambda x: nlp_utils.get_clean(x))
df.head()

"""## 3. Disaster Words Visualization with Word Cloud"""

from wordcloud import WordCloud

real = nlp_utils.get_value_counts(df[df['target']==1], 'text')
real = ' '.join(real.index)
real

word_cloud = WordCloud(max_font_size=100).generate(real)
plt.imshow(word_cloud)
plt.axis('off')
plt.show()

not_real = nlp_utils.get_value_counts(df[df['target']==0], 'text')
not_real = ' '.join(not_real.index)
not_real

word_cloud = WordCloud(max_font_size=100).generate(not_real)
plt.imshow(word_cloud)
plt.axis('off')
plt.show()

"""## 4. Classification with TFIDF and SVM

### 4.1. Training process
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.svm import LinearSVC

text = df['text']
y = df['target']
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(text)
X.shape

X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

def run_SVM(clf, X_train, X_test, y_train, y_test):
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)

  print('\nClassification Report')
  print(classification_report(y_test, y_pred))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# clf = LinearSVC()
# run_SVM(clf, X_train, X_test, y_train, y_test)
#

"""### 4.2. Test on competition data"""

df_test = pd.read_csv('test.csv')
df_test['text'] = df_test['text'].apply(lambda x: nlp_utils.get_clean(x))
X_submission_test = tfidf.transform(df_test['text'])
y_submission_pred = clf.predict(X_submission_test)
df_test['target'] = y_submission_pred
df_test[['id', 'target']].to_csv('predictions_svm_tfidf.csv', index=False)
df_test.head()

"""## 5. Classification with Word2Vec and SVM

### 5.1. Training process
"""

!python -m spacy download en_core_web_lg

import spacy
import en_core_web_lg
import numpy as np

nlp = en_core_web_lg.load()

x = 'apple oragne cloud'
doc = nlp(x)
doc.vector.shape

def get_vec(x):
    return nlp(x).vector

df['vec'] = df['text'].apply(lambda x: get_vec(x))
df.head()

X = df['vec'].to_numpy()
X = X.reshape(-1, 1)
X.shape

X = np.concatenate(np.concatenate(X, axis=0), axis=0).reshape(-1, 300)
X.shape

y = df['target']
y.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# clf = LinearSVC()
# run_SVM(clf, X_train, X_test, y_train, y_test)

"""### 5.2. Test on competition data"""

df_test = pd.read_csv('test.csv')
df_test['text'] = df_test['text'].apply(lambda x: nlp_utils.get_clean(x))
df_test['vec'] = df_test['text'].apply(lambda x: get_vec(x))
X_submission_test = df_test['vec'].to_numpy()
X_submission_test = X_submission_test.reshape(-1, 1)
X_submission_test = np.concatenate(np.concatenate(X_submission_test, axis=0), axis=0).reshape(-1, 300)
y_submission_pred = clf.predict(X_submission_test)
df_test['target'] = y_submission_pred
df_test[['id', 'target']].to_csv('predictions_svm_word2vec.csv', index=False)
df_test.head()

"""## 6. Word Embeddings and Classification wtih Deep Learning

### 6.1. Training process

#### 6.1.1 Data preparation
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout, Bidirectional, LSTM, BatchNormalization, Conv1D, MaxPooling1D, GlobalMaxPooling1D

text = df['text']
text

token = Tokenizer()
token.fit_on_texts(text)

vocab_size = len(token.word_index) + 1
vocab_size

print(token.word_index)

encoded_text = token.texts_to_sequences(text)
print(encoded_text)

longest_text = max(encoded_text, key=len)
len(longest_text)

max_length = len(longest_text) + 10
X = pad_sequences(encoded_text, maxlen=max_length, padding='post')
print(X)

X.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

"""#### 6.1.2 Base model"""

vec_size = 100

model_base = Sequential()
model_base.add(Embedding(vocab_size, vec_size, input_length=max_length))
model_base.add(Conv1D(32, 2, activation='relu'))
model_base.add(MaxPooling1D(2))
model_base.add(Dropout(0.5))
model_base.add(Dense(32, activation='relu'))
model_base.add(Dropout(0.5))
model_base.add(Dense(16, activation='relu'))
model_base.add(GlobalMaxPooling1D())
model_base.add(Dense(1, activation='sigmoid'))
model_base.summary()

"""#### 6.1.3 Model with an Additional Conv1D Layer
This model adds an extra Conv1D layer to enhance the model's ability to detect more complex patterns.
"""

vec_size = 100

model_conv1d = Sequential()
model_conv1d.add(Embedding(vocab_size, vec_size, input_length=max_length))
model_conv1d.add(Conv1D(32, 2, activation='relu'))
model_conv1d.add(MaxPooling1D(2))
model_conv1d.add(Dropout(0.5))
model_conv1d.add(Conv1D(64, 3, activation='relu'))
model_conv1d.add(MaxPooling1D(2))
model_conv1d.add(Dropout(0.5))
model_conv1d.add(Dense(32, activation='relu'))
model_conv1d.add(Dense(16, activation='relu'))
model_conv1d.add(GlobalMaxPooling1D())
model_conv1d.add(Dense(1, activation='sigmoid'))

model_conv1d.summary()

"""#### 6.1.4 Model with a Bidirectional LSTM Layer
This model replaces the Conv1D layer with a Bidirectional LSTM layer to better capture the sequential context of tweets.
"""

vec_size = 100

model_bilstm = Sequential()
model_bilstm.add(Embedding(vocab_size, vec_size, input_length=max_length))
model_bilstm.add(Bidirectional(LSTM(64, return_sequences=True)))
model_bilstm.add(GlobalMaxPooling1D())
model_bilstm.add(Dropout(0.5))
model_bilstm.add(Dense(32, activation='relu'))
model_bilstm.add(Dropout(0.5))
model_bilstm.add(Dense(16, activation='relu'))
model_bilstm.add(Dense(1, activation='sigmoid'))

model_bilstm.summary()

"""#### 6.1.5 Model with Batch Normalization
This model adds a BatchNormalization layer to stabilize and accelerate the learning process.
"""

vec_size = 100

model_batchnorm = Sequential()
model_batchnorm.add(Embedding(vocab_size, vec_size, input_length=max_length))
model_batchnorm.add(Conv1D(32, 2, activation='relu'))
model_batchnorm.add(MaxPooling1D(2))
model_batchnorm.add(Dropout(0.5))
model_batchnorm.add(BatchNormalization())
model_batchnorm.add(Dense(32, activation='relu'))
model_batchnorm.add(Dropout(0.5))
model_batchnorm.add(Dense(16, activation='relu'))
model_batchnorm.add(GlobalMaxPooling1D())
model_batchnorm.add(Dense(1, activation='sigmoid'))

model_batchnorm.summary()

"""#### 6.1.6 Models evaluation"""

def plot_learningCurve(history, epoch):
  # Plot training & validation accuracy values
  epoch_range = range(1, epoch+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc='upper left')
  plt.show()

  # Plot training & validation loss values
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc='upper left')
  plt.show()

def compile_fit_and_plot_model(model, X_train, X_test, y_train, y_test, epochs):
  config = model.get_config()
  clean_model = Sequential.from_config(config)
  clean_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
  history = clean_model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))
  plot_learningCurve(history, epochs)
  return clean_model, history

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trained_model_base, training_history_base = compile_fit_and_plot_model(model_base, X_train, X_test, y_train, y_test, 20)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trained_model_conv1d, training_history_conv1d = compile_fit_and_plot_model(model_conv1d, X_train, X_test, y_train, y_test, 20)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trained_model_bilstm, training_history_bilstm = compile_fit_and_plot_model(model_bilstm, X_train, X_test, y_train, y_test, 20)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trained_model_batchnorm, training_history_batchnorm = compile_fit_and_plot_model(model_batchnorm, X_train, X_test, y_train, y_test, 20)

"""#### 6.1.7 Best model training"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# best_model, best_model_hisotry = compile_fit_and_plot_model(model_conv1d, X_train, X_test, y_train, y_test, 2)

"""### 6.2 Test on competition data"""

def get_encoded(x):
  x = nlp_utils.get_clean(x)
  x = token.texts_to_sequences([x])
  x = pad_sequences(x, maxlen=max_length, padding='post')
  return x

def get_binary_classification(prediction, threshold=0.5):
  if prediction > threshold:
    return 1
  else:
    return 0

"""#### 6.2.1 Sample data observation"""

x = 'i am reportin earthquake in New York!'
vec = get_encoded(x)
vec

prediction = best_model.predict(vec)
prediction

test_df = pd.read_csv('test.csv')
test_df.head()

sample_test_df = test_df.sample(100)
sample_test_df

sample_test_df['prediction'] = sample_test_df['text'].apply(lambda x: best_model.predict(get_encoded(x)))
sample_test_df

sample_test_df['binary_prediction'] = sample_test_df['prediction'].apply(lambda x: get_binary_classification(x))
sample_test_df.sample(10)

"""#### 6.2.3 Final Test data predictions"""

test_df['prediction'] = test_df['text'].apply(lambda x: best_model.predict(get_encoded(x)))
test_df['binary_prediction'] = test_df['prediction'].apply(lambda x: get_binary_classification(x))
test_df.sample(10)

submission_df = test_df[['id', 'binary_prediction']]
submission_df.rename(columns={'binary_prediction': 'target'}, inplace=True)
submission_df.to_csv('predictions_deep_learning.csv', index=False)

